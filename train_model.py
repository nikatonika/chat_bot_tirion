import pandas as pd
import torch
from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
file_path = "data/game-of-thrones.csv"
df = pd.read_csv(file_path)

# –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–ø–ª–∏–∫–∏ –¢–∏—Ä–∏–æ–Ω–∞
tyrion_lines = df[df["Speaker"] == "TYRION"]["Text"].dropna().tolist()

# –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä—ã —Ä–µ–ø–ª–∏–∫ (–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç)
train_data = [InputExample(texts=[tyrion_lines[i], tyrion_lines[i+1]], label=1.0) for i in range(len(tyrion_lines)-1)]

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é Sentence-BERT –º–æ–¥–µ–ª—å
model = SentenceTransformer("all-MiniLM-L6-v2").to(device)

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º DataLoader
train_dataloader = DataLoader(train_data, batch_size=8, shuffle=True)
train_loss = losses.CosineSimilarityLoss(model)

# –û—Ü–µ–Ω–∫–∞ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(train_data[:100], name="tyrion-eval")

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
epochs = 5
warmup_steps = int(len(train_dataloader) * epochs * 0.1)
history = {"loss": []}

for epoch in range(epochs):
    print(f"Epoch {epoch+1}/{epochs}")

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=1,
        warmup_steps=warmup_steps,
        evaluator=evaluator,
        output_path="fine_tuned_tyrion_model"
    )

    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (InputExample)
    features = [InputExample(texts=[tyrion_lines[i], tyrion_lines[i+1]], label=1.0) for i in range(10)]

    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø–æ—Ç–µ—Ä—å
    # –ö–æ–¥–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏)
    encoded_sentences_1 = model.encode([tyrion_lines[i] for i in range(10)], convert_to_tensor=True)
    encoded_sentences_2 = model.encode([tyrion_lines[i+1] for i in range(10)], convert_to_tensor=True)

    # –í—ã—á–∏—Å–ª—è–µ–º –æ—à–∏–±–∫—É –ø–æ—Ç–µ—Ä—å —Å `CosineSimilarityLoss`
    with torch.no_grad():
        loss_value = train_loss(encoded_sentences_1, encoded_sentences_2).item()

    print(f"Loss: {loss_value:.4f}")


    print(f"Loss: {loss_value:.4f}")


    history["loss"].append(loss_value)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
model.save("fine_tuned_tyrion_model")
print("–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")

# üìä –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è
plt.figure(figsize=(10, 5))
plt.plot(history["loss"], label="Loss", marker="o")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.title("–ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏")
plt.grid(True)
plt.show()
